# -*- coding: utf-8 -*-
"""KDM3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wTY9K9aiJhQ0s3VyMTcv4VrRfQCX4dkm
"""

import nltk
nltk.download('all')
from nltk.corpus import wordnet

var1 = wordnet.synset('game.n.01') # Passing the synonym set to the variables to perform operations
var2 = wordnet.synset('tree.n.01')
var3 = wordnet.synset('atom.n.01')
var4 = wordnet.synset('hydrogen.n.01')
var5 = wordnet.synset('snore.v.01')
var6 = wordnet.synset('eat.v.01')

# Hypernym is a word that denotes general category.
# Hyponym is a word that denotes particular item from general category.
# Meronym — denotes a part of something. For meronyms we can take advantage of two NLTK’s functions: 
# a) part_meronyms() - obtains parts;  b) substance_meronyms() - obtains substances
# Holonym denotes a membership to something
# Entailment — denotes how verbs are involved (Relationship specific to verbs)
print('\n')
print('Hypernyms ->')

# For all the below operations, I'm printing only name using lemma.name() funtion.

print([lemma.name() for synset in var1.hypernyms() for lemma in synset.lemmas()]) 
print('\n')
print('Hyponyms ->')
print([lemma.name() for synset in var1.hyponyms() for lemma in synset.lemmas()])
print('\n')
print('Meronyms -> ')
print([lemma.name() for synset in var2.part_meronyms() for lemma in synset.lemmas()])
print([lemma.name() for synset in var2.substance_meronyms() for lemma in synset.lemmas()])
print('\n')
print('Holonyms ->')
print([lemma.name() for synset in var3.part_holonyms() for lemma in synset.lemmas()])
print([lemma.name() for synset in var4.substance_holonyms() for lemma in synset.lemmas()])
print('\n')
print('Entailment -> ')
print([lemma.name() for synset in var5.entailments() for lemma in synset.lemmas()])
print([lemma.name() for synset in var6.entailments() for lemma in synset.lemmas()])
print('\n')

"""# **Triplet Extraction**"""

!pip install textacy
import spacy
import textacy  # Importing 'textacy' library to perform the triplet operation

nlp = spacy.load('en_core_web_sm')
data = "Tom Mitchell wrote a new AI book. Users liked the content"

for content in data.split("."):  # First, I'm splitting the data into sentences using split function
  sentence = nlp (content)
  print('\n')
  print(sentence)
  triplets = textacy.extract.subject_verb_object_triples(sentence) # Passing the sentences to the textacy function and storing in triplets variable.
  print(triplets)
  for i in triplets:
    triplets_list = []   # creating final list to print the final triplet
    tuples_add = list(i)
    triplets_list.append(tuples_add) # Appending the extracted words to the final list
    print(triplets_list)